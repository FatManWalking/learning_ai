{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c15a88f-185b-4c84-996e-d64fed4c7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9999397b-144c-46a2-95bb-54b5981007e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import torch.autograd as autograd\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3b6360-5e81-474d-af27-6500c1599ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86267ac5-2005-4994-bbce-b4a694a410e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used Parameters\n",
    "\n",
    "lr= 23e-4\n",
    "epochs = 100\n",
    "resultion = (64,64)\n",
    "load_model = False\n",
    "train = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b2a2f1-0fb1-4351-8f24-742205765191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vae_0.0023_100_(64, 64)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = f'vae_{lr}_{epochs}_{resultion}'\n",
    "path2 = f'vae_{lr}_{100}_{resultion}'\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfeaf4f5-b0fb-4934-a6df-54cef1a6c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path.cwd().joinpath('data/pixelchars')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be80cf07-0859-4c26-9f88-ac920e043224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('B:/DHBW/learning_ai/VAE/models/vae_0.0023_100_(64, 64).pth')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set where to save the model\n",
    "model_path = Path.cwd().joinpath('models',f'{path}.pth')\n",
    "model_path2 = Path.cwd().joinpath('models',f'{path2}.pth')\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d114ce62-c2c5-44a2-bc89-7663ec162a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(resultion),\n",
    "        transforms.ToTensor()\n",
    "])\n",
    "\n",
    "batch_size = 100\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "data = ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "loader = DataLoader(dataset=data, batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d530647e-da44-479c-a67e-29478c20fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "108dac6b-15dd-4a94-9d59-363067b760db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c426da94-17c3-4169-8abc-6c0e00cbe26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, resultion=(64,64)):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.input_shape = (image_channels, ) + resultion\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        self.encoded_shape = self.feature_size()\n",
    "        self.fc1 = nn.Sequential(\n",
    "                        nn.Linear(self.encoded_shape, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128, 32)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "                        nn.Linear(self.encoded_shape, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128, 32)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "                        nn.Linear(32, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128, self.encoded_shape)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(self.encoded_shape, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        #print([a for a in x])\n",
    "        h = self.encoder(x)\n",
    "        #print(\"h\",h)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        #print(\"z\",z)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar\n",
    "    \n",
    "    def initialize_weights(self, layer):\n",
    "        \"\"\"initalizes the weights and biases\n",
    "        initalizes the weights and biases in case there was no state_dict provided to load them from\n",
    "        model_apply() iterativly goes through every layer and calls this methode\n",
    "        \n",
    "        Args:\n",
    "            layer (nn.Layer): the layer to be initalized\n",
    "        \"\"\"\n",
    "        # gain = nn.init.calculate_gain(self.cfg.nonlinearity)\n",
    "        init_weight = 'xavier_uniform'\n",
    "        gain = 1\n",
    "\n",
    "        if init_weight == 'orthogonal':\n",
    "            if type(layer) == nn.Conv2d:\n",
    "                nn.init.orthogonal_(layer.weight.data, gain=gain)\n",
    "            elif type(layer) == nn.Linear:\n",
    "                nn.init.orthogonal_(layer.weight.data, gain=gain)\n",
    "                layer.bias.data.fill_(0)\n",
    "            else:\n",
    "                pass\n",
    "        elif init_weight == 'xavier_uniform':\n",
    "            if type(layer) == nn.Conv2d:\n",
    "                nn.init.xavier_uniform_(layer.weight.data, gain=gain)\n",
    "\n",
    "            elif type(layer) == nn.Linear:\n",
    "                nn.init.xavier_uniform_(layer.weight.data, gain=gain)\n",
    "                layer.bias.data.fill_(0)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    def feature_size(self):\n",
    "        \"\"\"dynamically sets the dimension of the first linear Layer\n",
    "        \n",
    "        by calling this a forward pass through the layers definied in features is performed\n",
    "        and the resulting output_size can be used in the first linear Layer. Effectivly making\n",
    "        the net independend of the chosen resultion\n",
    "        Returns:\n",
    "            int: size of the flattend ccn output before linear layers\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.encoder(autograd.Variable(torch.zeros(1, * self.input_shape))).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af6a8f49-f730-4fff-bb5b-59806276287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_x, _ = next(iter(loader))\n",
    "image_channels = fixed_x.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9404f08-d71b-4dca-b9c1-67d9eaf4fd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (7): ReLU()\n",
       "    (8): Flatten()\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  )\n",
       "  (fc3): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=1024, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): UnFlatten()\n",
       "    (1): ConvTranspose2d(1024, 128, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): ConvTranspose2d(64, 32, kernel_size=(6, 6), stride=(2, 2))\n",
       "    (6): ReLU()\n",
       "    (7): ConvTranspose2d(32, 3, kernel_size=(6, 6), stride=(2, 2))\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE(image_channels=image_channels, resultion=resultion).to(device)\n",
    "\n",
    "if load_model:\n",
    "    print(\"Loading model from: \", model_path)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    # Applies a inital weight to Conv and Linear Layers\n",
    "    # Currently Xavier Uniform. Orthogonal is also implemented\n",
    "    model.apply(model.initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "521b598d-7b11-401c-ac1b-7bc16d93b87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1243"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg = Path.cwd().joinpath('models',f'{path}.txt')\n",
    "with open(model_cfg, 'w') as f:\n",
    "    f.write(model.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b669c80-e704-4468-9971-9202ffca13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc602f87-dbcb-4b88-9fd5-997a04301cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(x):\n",
    "    recon_x, _, _ = vae(x)\n",
    "    return torch.cat([x, recon_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c819936c-5796-431b-89dd-180056f8ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    \n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    # BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    #for i in [BCE + KLD, BCE, KLD]:\n",
    "    #        print(i.shape, i)\n",
    "\n",
    "    return BCE + KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94abd51f-cf0e-4e88-b2c0-5bf11f898fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjoer\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] Loss: 1149.587 1149.587 0.000\n",
      "Epoch[2/100] Loss: 1056.040 1056.039 0.001\n",
      "Epoch[3/100] Loss: 1081.371 1081.362 0.010\n",
      "Epoch[4/100] Loss: 1081.397 1081.379 0.018\n",
      "Epoch[5/100] Loss: 1041.720 1041.695 0.025\n",
      "Epoch[6/100] Loss: 1056.839 1056.813 0.026\n",
      "Epoch[7/100] Loss: 1004.084 1004.028 0.056\n",
      "Epoch[8/100] Loss: 1003.259 1003.185 0.074\n",
      "Epoch[9/100] Loss: 1004.864 1004.797 0.067\n",
      "Epoch[10/100] Loss: 940.921 940.845 0.076\n",
      "Epoch[11/100] Loss: 955.723 955.642 0.081\n",
      "Epoch[12/100] Loss: 933.194 933.108 0.086\n",
      "Epoch[13/100] Loss: 909.320 909.230 0.090\n",
      "Epoch[14/100] Loss: 934.451 934.361 0.089\n",
      "Epoch[15/100] Loss: 926.239 926.134 0.105\n",
      "Epoch[16/100] Loss: 938.487 938.384 0.103\n",
      "Epoch[17/100] Loss: 896.955 896.848 0.106\n",
      "Epoch[18/100] Loss: 906.696 906.591 0.105\n",
      "Epoch[19/100] Loss: 930.796 930.694 0.102\n",
      "Epoch[20/100] Loss: 915.173 915.067 0.106\n",
      "Epoch[21/100] Loss: 891.792 891.687 0.106\n",
      "Epoch[22/100] Loss: 903.733 903.628 0.105\n",
      "Epoch[23/100] Loss: 889.441 889.339 0.102\n",
      "Epoch[24/100] Loss: 885.583 885.480 0.103\n",
      "Epoch[25/100] Loss: 883.647 883.532 0.114\n",
      "Epoch[26/100] Loss: 865.282 865.163 0.118\n",
      "Epoch[27/100] Loss: 890.648 890.543 0.105\n",
      "Epoch[28/100] Loss: 880.105 879.996 0.109\n",
      "Epoch[29/100] Loss: 870.148 870.039 0.109\n",
      "Epoch[30/100] Loss: 859.089 858.973 0.116\n",
      "Epoch[31/100] Loss: 853.361 853.244 0.118\n",
      "Epoch[32/100] Loss: 855.499 855.390 0.108\n",
      "Epoch[33/100] Loss: 850.135 850.018 0.117\n",
      "Epoch[34/100] Loss: 875.691 875.585 0.106\n",
      "Epoch[35/100] Loss: 879.613 879.498 0.115\n",
      "Epoch[36/100] Loss: 857.219 857.106 0.113\n",
      "Epoch[37/100] Loss: 853.616 853.502 0.114\n",
      "Epoch[38/100] Loss: 861.065 860.950 0.116\n",
      "Epoch[39/100] Loss: 833.845 833.721 0.123\n",
      "Epoch[40/100] Loss: 857.251 857.131 0.119\n",
      "Epoch[41/100] Loss: 856.663 856.550 0.113\n",
      "Epoch[42/100] Loss: 844.370 844.257 0.112\n",
      "Epoch[43/100] Loss: 853.725 853.614 0.111\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (images, _) in enumerate(loader):\n",
    "            #for image in images:\n",
    "            #    print(torch.isinf(image))\n",
    "            #print(idx)\n",
    "            images = images.to(device)\n",
    "            #try:\n",
    "            recon_images, mu, logvar = model(images)\n",
    "\n",
    "                #if recon_images == None:\n",
    "                #    continue\n",
    "            loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "            #except(RuntimeError):\n",
    "            #    print(f\"idx:{idx},images: {images},labels{_}\\n\\nrecon{recon_images}\")\n",
    "            #    hallo + 3\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        to_print = \"Epoch[{}/{}] Loss: {:.3f} {:.3f} {:.3f}\".format(epoch+1, \n",
    "                                epochs, loss.data/batch_size, bce.data/batch_size, kld.data/batch_size)\n",
    "        print(to_print)\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f019b4a-f175-4f16-8a2a-0b83716f8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(x):\n",
    "    recon_x, _, _ = model(x)\n",
    "    return torch.cat([x, recon_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fe1fa-5882-4bbc-9170-819178f4f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from IPython.display import Image\n",
    "\n",
    "fixed_x = data[randint(1500, 2000)][0].unsqueeze(0)\n",
    "compare_x = compare(fixed_x.to(device))\n",
    "\n",
    "image_path = Path.cwd().joinpath('images','decoded',f'{path}.png')\n",
    "save_image(compare_x.data.cpu(), image_path)\n",
    "display(Image(image_path, width=700, unconfined=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35835a0f-cbe0-4d0b-9dc1-fc15574372c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    noise = torch.randn(batch_size, 32).to(device)\n",
    "    generated_images = model.decode(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844dbea-3b32-4e5b-b632-351334bebb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "def imshow(img, idx=[0, 300, 0, 300]):\n",
    "    fig = plt.figure()\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    try:\n",
    "        npimg = img.cpu().numpy()\n",
    "        \n",
    "    except:\n",
    "        npimg = img.numpy()\n",
    "    \n",
    "    \n",
    "    print(npimg.shape)\n",
    "    print(np.transpose(npimg, (1, 2, 0)).shape)\n",
    "        \n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))[idx[0]:idx[1], idx[2]:idx[3], :])\n",
    "    #plt.imshow(npimg)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41c790-01aa-42a9-a18a-73eab43598ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = generated_images.reshape((100, 3, 64, 64))\n",
    "imshow(torchvision.utils.make_grid(generated_images))\n",
    "image_path = Path.cwd().joinpath('images','generated',f'{path}.png')\n",
    "save_image(generated_images.data.cpu(), image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa68ced-e643-44cf-93b4-ec916f02e8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
